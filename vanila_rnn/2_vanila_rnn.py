# -*- coding: utf-8 -*-
"""2_Vanila_RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vrumAY7Thq4O03JkSP8csFR4eDJVJCUJ

# 2. Vanila RNN ëª¨ë¸ë§

- Vanila RNN : ì€ë‹‰ ë²¡í„°ê°€ ë‹¤ìŒ ì‹œì ì˜ ì…ë ¥ë²¡í„°ë¡œ ë‹¤ì‹œ ë“¤ì–´ê°€ëŠ” ê³¼ì •ì´ ìˆëŠ” ê¸°ë³¸ì ì¸ ìˆœí™˜ ì‹ ê²½ë§ ëª¨ë¸

- ë³‘ë ¬í™”ê°€ ë¶ˆê°€ëŠ¥ì— GPU ì—°ì‚°ì— ì´ì ì´ ì—†ìŒ -> Transformer ëª¨ë¸
- ê¸°ìš¸ê¸° ì†Œì‹¤ë¬¸ì œ -> LSTM, GRU, Attentionìœ¼ë¡œ í•´ê²°
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
# %matplotlib inline
import matplotlib.pyplot as plt
import urllib.request

# Deep Learning Module
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/spam_prc.csv')
data= data[data.columns[1:]]

"""## 2-1. `X_data`, `y_data` ë¶„ë¦¬

X_DATAì™€ Y_DATAë¥¼ ë¶„ë¦¬í•˜ê¸°ì „, ham, spamì„ 0ê³¼ 1ì˜ ì •ìˆ˜ë¡œ ë³€ê²½í•˜ì—¬ í•™ìŠµì„ íš¨ìœ¨ì ìœ¼ë¡œ ì§„í–‰í•˜ê²Œ ë§Œë“ ë‹¤.
"""

data['category'] = data.category.map({'ham':0, 'spam':1})

X_data = data['text']
y_data = data['category']
print('Mail: {}'.format(len(X_data)))
print('Target: {}'.format(len(y_data)))

"""## 2-2. Tokenizerë¥¼ ì´ìš©í•œ í† í°í™”"""

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_data) # í† í°í™”ë¥¼ ì§„í–‰í•œë‹¤.
sequences = tokenizer.texts_to_sequences(X_data) # ë‹¨ì–´ë¥¼ ì •ìˆ˜, ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•œë‹¤.

"""ìœ„ ì²˜ëŸ¼ ê° ë‹¨ì–´ë“¤ì— ë§¤í•‘í•˜ëŠ” ì •ìˆ˜ë¡œ ì¸ì½”ë”©í–ˆë‹¤. 
**3ê°œì˜ ë¬¸ìë©”ì„¸ì§€**ë§Œ ì¶œë ¥í•˜ì—¬ í™•ì¸í•´ë³´ì•˜ë‹¤.
"""

print(sequences[:3])

word_to_index = tokenizer.word_index

print(word_to_index)

"""`X_data`ì˜ ì¡´ì¬í•˜ëŠ” ëª¨ë“  ë‹¨ì–´ì™€ ì¸ë±ìŠ¤ë¥¼ ë¦¬í„´í•œë‹¤.

- ìœ„ ë‹¨ì–´ì˜ ê¸°ì¤€ì€ ë¹ˆë„ìˆ˜ê°€ ë†’ì„ ìˆ˜ë¡ ë‚®ì€ ì •ìˆ˜ê°€ ë¶€ì—¬ë˜ì–´ìˆë‹¤.
- ë¹ˆë„ìˆ˜ê°€ ê°€ì¥ ë†’ì€ `i`ëŠ” ê°€ì¥ ë‚®ì€ 1ìœ¼ë¡œ ë¶€ì—¬ë˜ì–´ ìˆë‹¤.

> ğŸ’¡ ì¶”ê°€ë¡œ, **ê° ë‹¨ì–´ë“¤ì˜ ë¹ˆë„ìˆ˜**ë¥¼ í™•ì¸í•´ ë³´ê¸°ë¡œ í–ˆë‹¤.
 
`tokenizer.word_count_items()`ë¥¼ ì¶œë ¥í•˜ì—¬ ë¹ˆë„ìˆ˜ì— ë”°ë¥¸ ë‹¨ì–´ë“¤ì˜ ë¹„ì¤‘ì„ í™•ì¸í•´ë³¸ë‹¤. 
"""

tokenizer.word_counts.items()

threshold = 2
total_cnt = len(word_to_index) # ë‹¨ì–´ì˜ ìˆ˜
rare_cnt = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì¹´ìš´íŠ¸
total_freq = 0 # í›ˆë ¨ ë°ì´í„°ì˜ ì „ì²´ ë‹¨ì–´ ë¹ˆë„ìˆ˜ ì´ í•©
rare_freq = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ì˜ ì´ í•©

# ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ì˜ ìŒ(pair)ì„ keyì™€ valueë¡œ ë°›ëŠ”ë‹¤.
for key, value in tokenizer.word_counts.items():
    total_freq = total_freq + value

    # ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ìœ¼ë©´
    if(value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value


print(f"- ë“±ì¥ ë¹ˆë„ê°€ {threshold - 1} ì •ë„ì¸ í¬ê·€í•œ ë‹¨ì–´ë“¤ì˜ ìˆ˜ : {rare_cnt} ")
print("- ë‹¨ì–´ ì§‘í•© ì¤‘ í¬ê·€ ë‹¨ì–´ë“¤ì˜ ë¹„ìœ¨ (%): ", round((rare_cnt / total_cnt)*100, 2))
print("- ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ê°€ ë“±ì¥í•  ë¹ˆë„ (%):", round((rare_freq / total_freq)*100, 2))

"""ë“±ì¥ ë¹ˆë„ê°€ 1ë°–ì— ì•ˆë˜ëŠ” í¬ê·€í•œ ë‹¨ì–´ì˜ ë¹„ìœ¨ì´ ë°˜ ì´ìƒì„ ì°¨ì§€ í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œ ìˆ˜ ìˆë‹¤. ë°˜ë©´, ì „ì²´ ë°ì´í„°ì—ì„œ ë“±ì¥ ë¹ˆë„ëŠ” 6%ë°–ì— ì°¨ì§€ í•˜ì§€ ì•Šì•˜ë‹¤.

## 2-3. Train Test ë¶„ë¦¬

ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
> íŒ¨ë”© ì²˜ë¦¬ ì‹œ, 0ë²ˆë¶€í„° í† í° ë²ˆí˜¸ê°€ ë¶€ì—¬ë˜ë‹ˆ 1ì„ ì¶”ê°€í•˜ì—¬ ì €ì¥í•˜ì˜€ìŠµë‹ˆë‹¤.
"""

vocab_size = len(word_to_index) + 1
print('ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°: {}'.format((vocab_size)))

n_of_train = int(len(sequences) * 0.8)
n_of_test = int(len(sequences) - n_of_train)
print('Train Data :',n_of_train)
print('Test Data:',n_of_test)

"""í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ 8:2 ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ì—ˆìŠµë‹ˆë‹¤. 
> ì‹¤ì œë¡œ,  ë‹¨ìˆœ ê³„ì‚°ë§Œ ì§„í–‰í•œ ê²ƒì´ë¯€ë¡œ ë°ì´í„°ë¥¼ ì§€ê¸ˆ ë‚˜ëˆ„ì§€ ì•Šê³  ìŠ¤íŒ¸ ë©”ì¼ì„ ë¶„ë¥˜í•˜ê¸°ì „ì— X_test,X_train,y_test,y_trainìœ¼ë¡œ ë‚˜ëˆŒ ì˜ˆì •ì…ë‹ˆë‹¤.
"""

X_data = sequences
print('ë¬¸ìì˜ ìµœëŒ€ ê¸¸ì´ : %d' % max(len(l) for l in X_data))
print('ë¬¸ìì˜ í‰ê·  ê¸¸ì´ : %f' % (sum(map(len, X_data))/len(X_data)))
# ì „ì²´ ê¸€ììˆ˜ / ì „ì²´ ë¬¸ìì˜ ê°¯ìˆ˜


plt.hist([len(s) for s in X_data], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

"""ê°€ì¥ ê¸¸ì´ê°€ ê¸´ ë¬¸ì ë©”ì„¸ì§€ëŠ” 189ë¬¸ìì˜ ë©”ì„¸ì§€ì´ë©°, ëŒ€ì²´ì ìœ¼ë¡œ 50ê¸¸ì´ ì´í•˜ ì •ë„ ë˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤."""

max_len = 189
# ì „ì²´ ë°ì´í„°ì…‹ì˜ ê¸¸ì´ëŠ” max_lenìœ¼ë¡œ ë§ì¶¥ë‹ˆë‹¤.
data = pad_sequences(X_data, maxlen = max_len)
print("í›ˆë ¨ ë°ì´í„°ì˜ í¬ê¸°(shape): ", data.shape)

X_test = data[n_of_train:] 
y_test = np.array(y_data[n_of_train:]) 

X_train = data[:n_of_train] 
y_train = np.array(y_data[:n_of_train])

"""## ğŸ‘¨â€ğŸ’» 2-4. Vanila RNN ì„ ì´ìš©í•œ ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜"""

from tensorflow.keras.layers import SimpleRNN, Embedding, Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint # ì½œë°±í•¨ìˆ˜ë¡œ ê°€ì¥ ì •í™•ë„ê°€ ë†’ì€ ì‹œì ì— Checkpointë¥¼ ë‘ì–´ ì €ì¥í•˜ë ¤ê³  í•©ë‹ˆë‹¤.
import tensorflow as tf

checkpoint = ModelCheckpoint('/content/drive/MyDrive/rnn-ver.8', monitor='val_acc', save_best_only=True, verbose=1)
earlystopping = EarlyStopping(monitor='val_acc', patience=5, verbose=1)

model = Sequential()
model.add(Embedding(vocab_size, 32)) # ì„ë² ë”© ë²¡í„° ì°¨ì›  : 32
model.add(SimpleRNN(32)) # ì€ë‹‰ì¸µì˜ ì‚¬ì´ì¦ˆ : 32
model.add(Dense(1, activation='sigmoid')) # ì´ì§„ë¶„ë¥˜ -> 1ê°œì˜ ë…¸ë“œë¡œ ì¶œë ¥

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
history = model.fit(X_train, y_train, callbacks=[checkpoint, earlystopping], epochs=10, batch_size=64, validation_split=0.2)

print("-----í…ŒìŠ¤íŠ¸ ì •í™•ë„ : %.4f" % (model.evaluate(X_test, y_test)[1]))

epochs = range(1, len(history.history['acc']) + 1)
plt.plot(epochs, history.history['loss'])
plt.plot(epochs, history.history['val_loss'])
plt.title('Model Loss ver8')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

model.save( "/content/drive/MyDrive/rnn-model.8")

