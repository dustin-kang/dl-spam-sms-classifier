# -*- coding: utf-8 -*-
"""2_Vanila_RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vrumAY7Thq4O03JkSP8csFR4eDJVJCUJ

# 2. Vanila RNN 모델링

- Vanila RNN : 은닉 벡터가 다음 시점의 입력벡터로 다시 들어가는 과정이 있는 기본적인 순환 신경망 모델

- 병렬화가 불가능에 GPU 연산에 이점이 없음 -> Transformer 모델
- 기울기 소실문제 -> LSTM, GRU, Attention으로 해결
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
# %matplotlib inline
import matplotlib.pyplot as plt
import urllib.request

# Deep Learning Module
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/spam_prc.csv')
data= data[data.columns[1:]]

"""## 2-1. `X_data`, `y_data` 분리

X_DATA와 Y_DATA를 분리하기전, ham, spam을 0과 1의 정수로 변경하여 학습을 효율적으로 진행하게 만든다.
"""

data['category'] = data.category.map({'ham':0, 'spam':1})

X_data = data['text']
y_data = data['category']
print('Mail: {}'.format(len(X_data)))
print('Target: {}'.format(len(y_data)))

"""## 2-2. Tokenizer를 이용한 토큰화"""

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_data) # 토큰화를 진행한다.
sequences = tokenizer.texts_to_sequences(X_data) # 단어를 정수, 인덱스로 변환하여 저장한다.

"""위 처럼 각 단어들에 매핑하는 정수로 인코딩했다. 
**3개의 문자메세지**만 출력하여 확인해보았다.
"""

print(sequences[:3])

word_to_index = tokenizer.word_index

print(word_to_index)

"""`X_data`의 존재하는 모든 단어와 인덱스를 리턴한다.

- 위 단어의 기준은 빈도수가 높을 수록 낮은 정수가 부여되어있다.
- 빈도수가 가장 높은 `i`는 가장 낮은 1으로 부여되어 있다.

> 💡 추가로, **각 단어들의 빈도수**를 확인해 보기로 했다.
 
`tokenizer.word_count_items()`를 출력하여 빈도수에 따른 단어들의 비중을 확인해본다. 
"""

tokenizer.word_counts.items()

threshold = 2
total_cnt = len(word_to_index) # 단어의 수
rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트
total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합
rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합

# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.
for key, value in tokenizer.word_counts.items():
    total_freq = total_freq + value

    # 단어의 등장 빈도수가 threshold보다 작으면
    if(value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value


print(f"- 등장 빈도가 {threshold - 1} 정도인 희귀한 단어들의 수 : {rare_cnt} ")
print("- 단어 집합 중 희귀 단어들의 비율 (%): ", round((rare_cnt / total_cnt)*100, 2))
print("- 전체 등장 빈도에서 희귀 단어가 등장할 빈도 (%):", round((rare_freq / total_freq)*100, 2))

"""등장 빈도가 1밖에 안되는 희귀한 단어의 비율이 반 이상을 차지 하는 것으로 알 수 있다. 반면, 전체 데이터에서 등장 빈도는 6%밖에 차지 하지 않았다.

## 2-3. Train Test 분리

단어 집합의 크기를 저장합니다.
> 패딩 처리 시, 0번부터 토큰 번호가 부여되니 1을 추가하여 저장하였습니다.
"""

vocab_size = len(word_to_index) + 1
print('단어 집합의 크기: {}'.format((vocab_size)))

n_of_train = int(len(sequences) * 0.8)
n_of_test = int(len(sequences) - n_of_train)
print('Train Data :',n_of_train)
print('Test Data:',n_of_test)

"""훈련 데이터와 테스트 데이터를 8:2 비율로 나누었습니다. 
> 실제로,  단순 계산만 진행한 것이므로 데이터를 지금 나누지 않고 스팸 메일을 분류하기전에 X_test,X_train,y_test,y_train으로 나눌 예정입니다.
"""

X_data = sequences
print('문자의 최대 길이 : %d' % max(len(l) for l in X_data))
print('문자의 평균 길이 : %f' % (sum(map(len, X_data))/len(X_data)))
# 전체 글자수 / 전체 문자의 갯수


plt.hist([len(s) for s in X_data], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

"""가장 길이가 긴 문자 메세지는 189문자의 메세지이며, 대체적으로 50길이 이하 정도 되는 것으로 알 수 있습니다."""

max_len = 189
# 전체 데이터셋의 길이는 max_len으로 맞춥니다.
data = pad_sequences(X_data, maxlen = max_len)
print("훈련 데이터의 크기(shape): ", data.shape)

X_test = data[n_of_train:] 
y_test = np.array(y_data[n_of_train:]) 

X_train = data[:n_of_train] 
y_train = np.array(y_data[:n_of_train])

"""## 👨‍💻 2-4. Vanila RNN 을 이용한 스팸 메일 분류"""

from tensorflow.keras.layers import SimpleRNN, Embedding, Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint # 콜백함수로 가장 정확도가 높은 시점에 Checkpoint를 두어 저장하려고 합니다.
import tensorflow as tf

checkpoint = ModelCheckpoint('/content/drive/MyDrive/rnn-ver.8', monitor='val_acc', save_best_only=True, verbose=1)
earlystopping = EarlyStopping(monitor='val_acc', patience=5, verbose=1)

model = Sequential()
model.add(Embedding(vocab_size, 32)) # 임베딩 벡터 차원  : 32
model.add(SimpleRNN(32)) # 은닉층의 사이즈 : 32
model.add(Dense(1, activation='sigmoid')) # 이진분류 -> 1개의 노드로 출력

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
history = model.fit(X_train, y_train, callbacks=[checkpoint, earlystopping], epochs=10, batch_size=64, validation_split=0.2)

print("-----테스트 정확도 : %.4f" % (model.evaluate(X_test, y_test)[1]))

epochs = range(1, len(history.history['acc']) + 1)
plt.plot(epochs, history.history['loss'])
plt.plot(epochs, history.history['val_loss'])
plt.title('Model Loss ver8')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

model.save( "/content/drive/MyDrive/rnn-model.8")

